# LLM Code Validation Framework

This document provides a comprehensive guide for validating LLM-generated code within the Agentics system. It complements the [ARCHITECTURE_REFACTOR.md](ARCHITECTURE_REFACTOR.md) by focusing on systematic validation and testing of generated TypeScript code and Python agentics components.

## 1. Validation Framework Overview

The LLM Code Validation Framework ensures the quality, correctness, and safety of code generated by large language models. It consists of three primary validation components:

### Core Components

1. **Safe Execution Engine**: Provides sandboxed runtime environments for testing generated code without risking system integrity
2. **Test Validation System**: Runs and analyzes automated tests to verify code functionality and coverage
3. **LangChain Pattern Validator**: Ensures generated code follows LangChain best practices for composition, error handling, and state management

### Integration with Agentics Workflow

The validation framework integrates seamlessly with the collaborative code generation workflow:

```
Code Generation → Validation → Refinement → Integration
     ↓              ↓           ↓          ↓
   LLM Output → Safety Check → Test Run → Pattern Check
```

### Validation Pipeline

```python
class ValidationPipeline:
    """Orchestrates the complete validation process"""

    def __init__(self):
        self.safe_executor = SafeCodeExecutor()
        self.test_runner = TestValidationRunner()
        self.pattern_validator = LangChainPatternValidator()

    def validate(self, generated_code: str, test_code: str) -> ValidationReport:
        """Run complete validation suite"""
        # Phase 1: Safety validation
        safety_result = self.safe_executor.validate_code(generated_code)

        # Phase 2: Test execution
        test_result = self.test_runner.run_tests(test_code, generated_code)

        # Phase 3: Pattern validation
        pattern_result = self.pattern_validator.validate_patterns(generated_code)

        return ValidationReport(
            safety=safety_result,
            tests=test_result,
            patterns=pattern_result
        )
```

## 2. TypeScript Code Validation

TypeScript code validation ensures generated code is syntactically correct, type-safe, and executes without runtime errors.

### Compilation Validation

```typescript
interface CompilationResult {
  success: boolean;
  errors: Diagnostic[];
  warnings: Diagnostic[];
}

class TypeScriptValidator {
  validate_compilation(code: string): CompilationResult {
    const program = ts.createProgram([code], {
      target: ts.ScriptTarget.ES2020,
      module: ts.ModuleKind.CommonJS,
      strict: true,
      noImplicitAny: true,
      strictNullChecks: true
    });

    const diagnostics = ts.getPreEmitDiagnostics(program);
    return {
      success: diagnostics.length === 0,
      errors: diagnostics.filter(d => d.category === ts.DiagnosticCategory.Error),
      warnings: diagnostics.filter(d => d.category === ts.DiagnosticCategory.Warning)
    };
  }
}
```

### Runtime Safety Validation

```python
class SafeCodeExecutor:
    """Sandboxed TypeScript execution environment"""

    def __init__(self):
        self.node_process = None
        self.sandbox_config = {
            'timeout': 5000,  # 5 second execution limit
            'memory_limit': '128MB',
            'network_disabled': True,
            'filesystem_readonly': True
        }

    def execute_safely(self, code: str) -> ExecutionResult:
        """Execute code in isolated environment"""
        try:
            # Create temporary file with code
            temp_file = self._create_temp_file(code)

            # Execute with Node.js in sandbox
            result = subprocess.run(
                ['node', '--experimental-vm-modules', temp_file],
                capture_output=True,
                timeout=self.sandbox_config['timeout'],
                cwd='/tmp/sandbox'  # Isolated directory
            )

            return ExecutionResult(
                success=result.returncode == 0,
                output=result.stdout.decode(),
                errors=result.stderr.decode(),
                execution_time=result.duration
            )

        except subprocess.TimeoutExpired:
            return ExecutionResult(success=False, error="Execution timeout")
        except Exception as e:
            return ExecutionResult(success=False, error=str(e))
```

### Type Safety Analysis

```typescript
class TypeSafetyAnalyzer {
  analyze_types(code: string): TypeAnalysis {
    const checker = this._create_type_checker(code);

    return {
      any_types: this._count_any_usage(checker),
      null_checks: this._analyze_null_safety(checker),
      generic_usage: this._analyze_generics(checker),
      interface_compliance: this._check_interface_implementation(checker)
    };
  }
}
```

## 3. Test Validation Framework

The test validation framework ensures generated tests are comprehensive, correct, and provide adequate coverage.

### Jest Test Execution

```python
class JestTestRunner:
    """Executes Jest tests with coverage analysis"""

    def run_tests(self, test_code: str, source_code: str) -> TestResult:
        """Run Jest tests and collect results"""

        # Write test and source files
        self._write_test_files(test_code, source_code)

        # Configure Jest
        jest_config = {
            'collectCoverage': True,
            'coverageReporters': ['json', 'text'],
            'testTimeout': 10000,
            'setupFilesAfterEnv': ['<rootDir>/jest.setup.js']
        }

        # Execute tests
        result = subprocess.run(
            ['npx', 'jest', '--config', json.dumps(jest_config)],
            capture_output=True,
            cwd=self.test_directory
        )

        # Parse coverage
        coverage = self._parse_coverage_report()

        return TestResult(
            passed=result.returncode == 0,
            output=result.stdout.decode(),
            errors=result.stderr.decode(),
            coverage=coverage,
            execution_time=result.duration
        )
```

### Test Coverage Analysis

```python
class CoverageAnalyzer:
    """Analyzes test coverage metrics"""

    def analyze_coverage(self, coverage_data: dict) -> CoverageReport:
        """Analyze coverage report for quality metrics"""

        total_lines = coverage_data['total']['lines']['total']
        covered_lines = coverage_data['total']['lines']['covered']
        coverage_percentage = (covered_lines / total_lines) * 100

        return CoverageReport(
            line_coverage=coverage_percentage,
            branch_coverage=coverage_data['total']['branches']['pct'],
            function_coverage=coverage_data['total']['functions']['pct'],
            uncovered_lines=self._identify_uncovered_lines(coverage_data),
            quality_score=self._calculate_quality_score(coverage_data)
        )
```

### Assertion Validation

```typescript
class AssertionValidator {
  validate_assertions(test_code: string): AssertionAnalysis {
    const ast = this._parse_test_code(test_code);

    return {
      assertion_count: this._count_assertions(ast),
      assertion_types: this._categorize_assertions(ast),
      test_structure: this._analyze_test_structure(ast),
      mock_usage: this._analyze_mock_patterns(ast)
    };
  }
}
```

## 4. LangChain Best Practices Validation

Validates that generated Python agentics code follows LangChain patterns and best practices.

### Chain Composition Validation

```python
class ChainCompositionValidator:
    """Validates LangChain chain composition patterns"""

    def validate_chain(self, code: str) -> ChainValidation:
        """Validate chain composition and structure"""

        ast_tree = ast.parse(code)

        return ChainValidation(
            uses_lcel=self._check_lcel_usage(ast_tree),
            proper_error_handling=self._validate_error_handling(ast_tree),
            state_management=self._analyze_state_flow(ast_tree),
            tool_integration=self._check_tool_usage(ast_tree),
            composability_score=self._calculate_composability_score(ast_tree)
        )
```

### Error Handling Validation

```python
class ErrorHandlingValidator:
    """Validates error handling patterns in LangChain code"""

    def validate_error_patterns(self, code: str) -> ErrorValidation:
        """Analyze error handling implementation"""

        try_catch_blocks = self._find_try_catch_blocks(code)
        circuit_breakers = self._find_circuit_breakers(code)
        fallback_strategies = self._find_fallback_patterns(code)

        return ErrorValidation(
            try_catch_coverage=self._calculate_coverage(try_catch_blocks),
            circuit_breaker_usage=len(circuit_breakers) > 0,
            fallback_strategies=len(fallback_strategies),
            error_propagation=self._analyze_error_flow(code)
        )
```

### State Management Validation

```python
class StateManagementValidator:
    """Validates state management patterns"""

    def validate_state_handling(self, code: str) -> StateValidation:
        """Validate state management implementation"""

        state_classes = self._find_state_classes(code)
        immutability_patterns = self._check_immutability(code)
        transformation_methods = self._find_state_transformers(code)

        return StateValidation(
            immutable_state=len(state_classes) > 0,
            proper_transformations=len(transformation_methods) > 0,
            state_flow=self._analyze_state_flow(code),
            dataclasses_usage=self._check_dataclass_usage(code)
        )
```

## 5. Unit and Integration Testing Best Practices

LangChain-inspired best practices for testing Python agentics components.

### Unit Test Patterns

```python
# Example: Testing a LangChain Runnable
class TestCodeGeneratorAgent(unittest.TestCase):
    """Unit tests for CodeGeneratorAgent following LangChain patterns"""

    def setUp(self):
        """Set up test fixtures with proper mocking"""
        self.llm = MagicMock(spec=Runnable)
        self.agent = CodeGeneratorAgent(self.llm)
        self.test_state = CodeGenerationState(
            issue_url="https://github.com/test/repo/issues/1",
            ticket_content="Test ticket",
            requirements=["req1", "req2"],
            acceptance_criteria=["crit1"],
            code_spec=CodeSpec(language="typescript", framework="react"),
            test_spec=TestSpec(testing_framework="jest")
        )

    @patch('agents.code_generator_agent.CodeGeneratorAgent._create_prompt')
    def test_generate_code_success(self, mock_prompt):
        """Test successful code generation"""
        # Arrange
        expected_code = "function test() { return true; }"
        self.llm.invoke.return_value = AIMessage(content=expected_code)
        mock_prompt.return_value = "Generate code prompt"

        # Act
        result = self.agent.generate(self.test_state)

        # Assert
        self.assertIsNotNone(result.generated_code)
        self.llm.invoke.assert_called_once()
        mock_prompt.assert_called_once_with(self.test_state)

    def test_generate_code_with_invalid_state(self):
        """Test error handling for invalid state"""
        invalid_state = CodeGenerationState(
            issue_url="",
            ticket_content="",
            requirements=[],
            acceptance_criteria=[],
            code_spec=None,
            test_spec=None
        )

        with self.assertRaises(ValueError):
            self.agent.generate(invalid_state)
```

### Integration Test Patterns

```python
# Example: Testing agent composition
class TestAgentComposition(unittest.TestCase):
    """Integration tests for agent composition"""

    def setUp(self):
        """Set up composable agents for integration testing"""
        self.composer = AgentComposer()
        self.llm = MagicMock(spec=Runnable)

        # Register test agents
        self.composer.register_agent("code_gen", CodeGeneratorAgent(self.llm))
        self.composer.register_agent("test_gen", TestGeneratorAgent(self.llm))

    def test_collaborative_generation_workflow(self):
        """Test complete collaborative generation workflow"""
        # Arrange
        initial_state = CodeGenerationState(
            issue_url="https://github.com/test/repo/issues/1",
            ticket_content="Implement user authentication",
            requirements=["OAuth integration", "JWT tokens"],
            acceptance_criteria=["User can login", "Tokens are secure"],
            code_spec=CodeSpec(language="typescript", framework="express"),
            test_spec=TestSpec(testing_framework="jest")
        )

        # Act
        workflow = self.composer.create_workflow("code_test_gen", {
            "agents": ["code_gen", "test_gen"],
            "pattern": "collaborative"
        })

        result = workflow.invoke(initial_state)

        # Assert
        self.assertIsNotNone(result.generated_code)
        self.assertIsNotNone(result.generated_tests)
        self.assertIsNotNone(result.validation_results)

    @patch('agents.agent_composer.AgentComposer._execute_parallel')
    def test_parallel_processing(self, mock_parallel):
        """Test parallel agent execution"""
        mock_parallel.return_value = {"agent1": "result1", "agent2": "result2"}

        results = self.composer._execute_parallel(["agent1", "agent2"])

        self.assertEqual(len(results), 2)
        mock_parallel.assert_called_once()
```

### Mocking Best Practices

```python
class MockingPatterns:
    """Best practices for mocking in LangChain tests"""

    def create_llm_mock(self) -> MagicMock:
        """Create properly configured LLM mock"""
        llm_mock = MagicMock(spec=Runnable)
        llm_mock.invoke.return_value = AIMessage(content="Mock response")
        return llm_mock

    def create_tool_mock(self) -> MagicMock:
        """Create tool mock with proper interface"""
        tool_mock = MagicMock(spec=Tool)
        tool_mock.name = "test_tool"
        tool_mock.description = "Test tool description"
        tool_mock.invoke.return_value = "Tool result"
        return tool_mock

    def create_state_mock(self) -> CodeGenerationState:
        """Create realistic state mock for testing"""
        return CodeGenerationState(
            issue_url="https://github.com/test/repo/issues/123",
            ticket_content="Mock ticket for testing",
            requirements=["Mock requirement 1", "Mock requirement 2"],
            acceptance_criteria=["Mock criteria 1"],
            code_spec=CodeSpec(language="typescript", framework="react"),
            test_spec=TestSpec(testing_framework="jest")
        )
```

## 6. Validation Reports

Structured reports provide comprehensive feedback on validation results.

### Report Structure

```python
@dataclass
class ValidationReport:
    """Comprehensive validation report"""

    # Metadata
    timestamp: datetime
    code_hash: str
    validation_version: str

    # Validation Results
    safety_check: SafetyValidation
    test_results: TestValidation
    pattern_validation: PatternValidation

    # Quality Metrics
    overall_score: float  # 0-100
    risk_level: RiskLevel  # LOW, MEDIUM, HIGH, CRITICAL

    # Recommendations
    critical_issues: List[str]
    warnings: List[str]
    suggestions: List[str]

    def to_markdown(self) -> str:
        """Generate markdown report"""
        return f"""
# Validation Report

**Generated:** {self.timestamp}
**Code Hash:** {self.code_hash}
**Overall Score:** {self.overall_score}/100
**Risk Level:** {self.risk_level}

## Safety Validation
{self.safety_check.to_markdown()}

## Test Results
{self.test_results.to_markdown()}

## Pattern Validation
{self.pattern_validation.to_markdown()}

## Critical Issues
{chr(10).join(f"- {issue}" for issue in self.critical_issues)}

## Warnings
{chr(10).join(f"- {warning}" for warning in self.warnings)}

## Suggestions
{chr(10).join(f"- {suggestion}" for suggestion in self.suggestions)}
"""
```

### Quality Scoring

```python
class QualityScorer:
    """Calculates overall quality scores"""

    WEIGHTS = {
        'safety': 0.4,
        'test_coverage': 0.3,
        'pattern_compliance': 0.3
    }

    def calculate_score(self, report: ValidationReport) -> float:
        """Calculate weighted quality score"""

        safety_score = self._score_safety(report.safety_check)
        test_score = self._score_tests(report.test_results)
        pattern_score = self._score_patterns(report.pattern_validation)

        return (
            safety_score * self.WEIGHTS['safety'] +
            test_score * self.WEIGHTS['test_coverage'] +
            pattern_score * self.WEIGHTS['pattern_compliance']
        )

    def assess_risk(self, score: float) -> RiskLevel:
        """Determine risk level based on score"""
        if score >= 80:
            return RiskLevel.LOW
        elif score >= 60:
            return RiskLevel.MEDIUM
        elif score >= 40:
            return RiskLevel.HIGH
        else:
            return RiskLevel.CRITICAL
```

## 7. Implementation Examples

### Complete Validation Pipeline

```python
# validation_pipeline.py
from typing import Dict, Any
from .validators import (
    TypeScriptValidator,
    JestTestRunner,
    LangChainPatternValidator
)
from .reports import ValidationReport

class LLMCodeValidationPipeline:
    """Complete validation pipeline for LLM-generated code"""

    def __init__(self):
        self.ts_validator = TypeScriptValidator()
        self.test_runner = JestTestRunner()
        self.pattern_validator = LangChainPatternValidator()

    def validate_typescript_code(
        self,
        code: str,
        tests: str,
        context: Dict[str, Any]
    ) -> ValidationReport:
        """Validate TypeScript code with tests"""

        # 1. Compilation validation
        compile_result = self.ts_validator.validate_compilation(code)

        # 2. Runtime safety check
        safety_result = self.ts_validator.validate_runtime_safety(code)

        # 3. Test execution
        test_result = self.test_runner.run_tests(tests, code)

        # 4. Pattern validation (if Python agentics code)
        pattern_result = None
        if context.get('is_agentics_code'):
            pattern_result = self.pattern_validator.validate_patterns(code)

        # 5. Generate comprehensive report
        return ValidationReport.create(
            compilation=compile_result,
            safety=safety_result,
            tests=test_result,
            patterns=pattern_result,
            context=context
        )

# Usage example
if __name__ == "__main__":
    pipeline = LLMCodeValidationPipeline()

    # Example validation
    generated_code = """
    export class UserService {
        async getUser(id: string): Promise<User> {
            return await this.db.findUser(id);
        }
    }
    """

    test_code = """
    describe('UserService', () => {
        it('should get user', async () => {
            const service = new UserService();
            const user = await service.getUser('123');
            expect(user).toBeDefined();
        });
    });
    """

    report = pipeline.validate_typescript_code(
        generated_code,
        test_code,
        {'is_agentics_code': False}
    )

    print(report.to_markdown())
```

### Integration with Agentics Workflow

```python
# integration_example.py
from agentics.src.collaborative_generator import CollaborativeGenerator
from .validation_pipeline import LLMCodeValidationPipeline

class ValidatedCollaborativeGenerator(CollaborativeGenerator):
    """Collaborative generator with integrated validation"""

    def __init__(self, llm_reasoning, llm_code):
        super().__init__(llm_reasoning, llm_code)
        self.validator = LLMCodeValidationPipeline()

    def generate_with_validation(
        self,
        state: CodeGenerationState,
        max_iterations: int = 3
    ) -> CodeGenerationState:
        """Generate code with iterative validation"""

        current_state = state

        for iteration in range(max_iterations):
            # Generate code and tests
            generated_state = self.generate_collaboratively(current_state)

            # Validate the generated code
            validation_report = self.validator.validate_typescript_code(
                generated_state.generated_code,
                generated_state.generated_tests,
                {'iteration': iteration + 1}
            )

            # Check if validation passes
            if validation_report.overall_score >= 80:
                return generated_state.with_validation(validation_report)

            # Refine based on validation feedback
            current_state = self._refine_based_on_feedback(
                generated_state,
                validation_report
            )

        # Return best effort if max iterations reached
        return current_state
```

This validation framework ensures that all LLM-generated code meets high standards of quality, safety, and correctness before integration into the production system.