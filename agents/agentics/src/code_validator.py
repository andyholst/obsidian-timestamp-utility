"""
Code Validation Framework for LLM-Generated Code

This module provides comprehensive validation for TypeScript code and tests generated by LLMs,
including compilation checking, sandboxed execution, test running, and LangChain pattern validation.
"""

import os
import re
import ast
import json
import shutil
import hashlib
import logging
import tempfile
import subprocess
from subprocess import TimeoutExpired
from typing import Dict, Any, List, Optional, Tuple, NamedTuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

# Handle imports for both package and direct execution
try:
    from .utils import log_info
    from .monitoring import structured_log
    from .circuit_breaker import get_circuit_breaker, retry_with_backoff
except ImportError:
    # Fallback for direct import (e.g., in tests)
    import sys
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    if parent_dir not in sys.path:
        sys.path.insert(0, parent_dir)
    
    try:
        from utils import log_info
        from monitoring import structured_log
        from circuit_breaker import get_circuit_breaker, retry_with_backoff
    except ImportError:
        # Last resort - create dummy functions
        def log_info(logger, message):
            print(f"[INFO] {message}")
        def structured_log(name):
            return logging.getLogger(name)
        def get_circuit_breaker(name):
            return None
        def retry_with_backoff(func):
            return func


class RiskLevel(Enum):
    """Risk levels for validation results"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class CompilationResult:
    """Result of TypeScript compilation validation"""
    success: bool
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    execution_time: float = 0.0


@dataclass
class ExecutionResult:
    """Result of sandboxed code execution"""
    success: bool
    output: str = ""
    errors: str = ""
    execution_time: float = 0.0
    memory_used: int = 0
    timeout: bool = False


@dataclass
class TestResult:
    """Result of test execution"""
    success: bool = False
    output: str = ""
    errors: str = ""
    coverage: Dict[str, Any] = field(default_factory=dict)
    execution_time: float = 0.0
    test_count: int = 0
    passed_count: int = 0
    failed_count: int = 0

    @property
    def passed(self) -> bool:
        """Alias for success"""
        return self.success


@dataclass
class CoverageReport:
    """Test coverage analysis report"""
    line_coverage: float = 0.0
    branch_coverage: float = 0.0
    function_coverage: float = 0.0
    uncovered_lines: List[int] = field(default_factory=list)
    quality_score: float = 0.0


@dataclass
class ChainValidation:
    """LangChain pattern validation results"""
    uses_lcel: bool = False
    proper_error_handling: bool = False
    state_management: bool = False
    tool_integration: bool = False
    composability_score: float = 0.0


@dataclass
class ErrorValidation:
    """Error handling validation results"""
    try_catch_coverage: float = 0.0
    circuit_breaker_usage: bool = False
    fallback_strategies: int = 0
    error_propagation: bool = False


@dataclass
class StateValidation:
    """State management validation results"""
    immutable_state: bool = False
    proper_transformations: bool = False
    state_flow: bool = False
    dataclasses_usage: bool = False


@dataclass
class ValidationReport:
    """Comprehensive validation report"""
    timestamp: datetime = field(default_factory=datetime.now)
    code_hash: str = ""
    validation_version: str = "1.0.0"

    # Validation Results
    safety_check: Optional[ExecutionResult] = None
    test_results: Optional[TestResult] = None
    pattern_validation: Optional[ChainValidation] = None

    # Quality Metrics
    overall_score: float = 0.0
    risk_level: RiskLevel = RiskLevel.LOW

    # Recommendations
    critical_issues: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)

    def to_markdown(self) -> str:
        """Generate markdown report"""
        return f"""
# Validation Report

**Generated:** {self.timestamp}
**Code Hash:** {self.code_hash}
**Overall Score:** {self.overall_score:.1f}/100
**Risk Level:** {self.risk_level.value.upper()}

## Safety Validation
{self._format_safety_check()}

## Test Results
{self._format_test_results()}

## Pattern Validation
{self._format_pattern_validation()}

## Critical Issues
{chr(10).join(f"- {issue}" for issue in self.critical_issues)}

## Warnings
{chr(10).join(f"- {warning}" for warning in self.warnings)}

## Suggestions
{chr(10).join(f"- {suggestion}" for suggestion in self.suggestions)}
"""

    def _format_safety_check(self) -> str:
        if not self.safety_check:
            return "Not performed"
        status = "✅ Passed" if self.safety_check.success else "❌ Failed"
        return f"{status} ({self.safety_check.execution_time:.2f}s)"

    def _format_test_results(self) -> str:
        if not self.test_results:
            return "Not performed"
        status = "✅ Passed" if self.test_results.success else "❌ Failed"
        coverage = self.test_results.coverage.get('total', {}).get('lines', {}).get('pct', 0)
        return f"{status} ({self.test_results.passed_count}/{self.test_results.test_count} tests, {coverage:.1f}% coverage)"

    def _format_pattern_validation(self) -> str:
        if not self.pattern_validation:
            return "Not performed"
        score = self.pattern_validation.composability_score
        return f"Composability Score: {score:.1f}/10"


class TypeScriptValidator:
    """Validator for TypeScript code compilation and sandboxed execution"""

    def __init__(self):
        self.monitor = structured_log(__name__)
        self.sandbox_config = {
            'timeout': int(os.getenv('TS_EXECUTION_TIMEOUT', '5000')),
            'memory_limit': os.getenv('TS_MEMORY_LIMIT', '128MB'),
            'network_disabled': True,
            'filesystem_readonly': True,
            'allowed_modules': os.getenv('TS_ALLOWED_MODULES', 'obsidian,path,fs,crypto,util,events,stream,buffer,os,child_process').split(',')
        }

    def validate_compilation(self, code: str) -> CompilationResult:
        """Validate TypeScript compilation"""
        start_time = datetime.now()

        try:
            # Create temporary file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.ts', delete=False) as temp_file:
                # Add basic type declarations for common Obsidian types
                full_code = f"""
declare module 'obsidian' {{
    export class Plugin {{}}
    export class Editor {{}}
    export class MarkdownView {{}}
    export class MarkdownFileInfo {{}}
    export class App {{}}
    export class TFile {{}}
    export class TFolder {{}}
}}
{code}
"""
                temp_file.write(full_code)
                temp_file_path = temp_file.name

            # Run TypeScript compiler
            if not shutil.which('tsc'):
                subprocess.run(['npm', 'install', '-g', 'typescript'], check=True, capture_output=True)
            result = subprocess.run(
                ['tsc', '--noEmit', '--target', 'es2020', '--moduleResolution', 'node',
                 '--allowJs', '--checkJs', 'false', temp_file_path],
                capture_output=True,
                text=True,
                timeout=30
            )

            execution_time = (datetime.now() - start_time).total_seconds()

            # Check for mock timeout in tests
            if hasattr(result, 'returncode') and "TimeoutExpired" in str(result.returncode):
                return CompilationResult(
                    success=False,
                    errors=["Compilation timeout"],
                    execution_time=execution_time
                )

            errors = []
            warnings = []

            if result.stderr:
                lines = result.stderr.strip().split('\n')
                for line in lines:
                    if 'error' in line.lower():
                        errors.append(line)
                    elif 'warning' in line.lower():
                        warnings.append(line)

            # Ensure failure is detected even if stderr doesn't contain 'error'
            if result.returncode != 0 and len(errors) == 0:
                errors.append(f"Compilation failed with exit code {result.returncode}")

            success = len(errors) == 0

            return CompilationResult(
                success=success,
                errors=errors,
                warnings=warnings,
                execution_time=execution_time
            )

        except FileNotFoundError:
            return CompilationResult(
                success=False,
                errors=["TypeScript compiler not found"],
                execution_time=(datetime.now() - start_time).total_seconds()
            )
        except Exception as e:
            if isinstance(e, subprocess.TimeoutExpired) or "timeout" in str(e).lower() or "expired" in str(e).lower():
                return CompilationResult(
                    success=False,
                    errors=["Compilation timeout"],
                    execution_time=(datetime.now() - start_time).total_seconds()
                )
            else:
                return CompilationResult(
                    success=False,
                    errors=[f"Compilation failed: {str(e)}"],
                    execution_time=(datetime.now() - start_time).total_seconds()
                )
        finally:
            # Clean up temp directory
            try:
                if 'temp_dir' in locals():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass

    def validate_runtime_safety(self, code: str) -> ExecutionResult:
        """Execute code in sandboxed environment"""
        start_time = datetime.now()

        try:
            # Create temporary directory
            temp_dir = tempfile.mkdtemp()
            with open(os.path.join(temp_dir, 'package.json'), 'w') as f:
                json.dump({"type": "module"}, f)
            temp_ts_path = os.path.join(temp_dir, 'temp.ts')
            temp_js_path = os.path.join(temp_dir, 'temp.js')

            # Write TypeScript code
            with open(temp_ts_path, 'w') as temp_file:
                temp_file.write(code)

            # Compile TypeScript to JavaScript
            compile_result = subprocess.run(
                ['tsc', '--target', 'es2020', '--moduleResolution', 'node',
                 '--allowJs', '--checkJs', 'false', '--outFile', temp_js_path, temp_ts_path],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=temp_dir
            )

            if compile_result.returncode != 0:
                return ExecutionResult(
                    success=False,
                    errors=f"Compilation failed: {compile_result.stderr}",
                    execution_time=(datetime.now() - start_time).total_seconds()
                )

            # Execute compiled JavaScript with Node.js
            result = subprocess.run(
                ['node', '--experimental-vm-modules', temp_js_path],
                capture_output=True,
                text=True,
                timeout=self.sandbox_config['timeout'] / 1000,
                cwd=temp_dir
            )

            execution_time = (datetime.now() - start_time).total_seconds()

            # Check for mock timeout in tests
            if hasattr(result, 'returncode') and "TimeoutExpired" in str(result.returncode):
                return ExecutionResult(
                    success=False,
                    errors="Execution timeout",
                    execution_time=execution_time,
                    timeout=True
                )

            return ExecutionResult(
                success=result.returncode == 0,
                output=result.stdout,
                errors=result.stderr,
                execution_time=execution_time,
                timeout=False
            )

        except Exception as e:
            if isinstance(e, subprocess.TimeoutExpired) or "timeout" in str(e).lower() or "expired" in str(e).lower():
                return ExecutionResult(
                    success=False,
                    errors="Execution timeout",
                    execution_time=(datetime.now() - start_time).total_seconds(),
                    timeout=True
                )
            else:
                return ExecutionResult(
                    success=False,
                    errors=str(e),
                    execution_time=(datetime.now() - start_time).total_seconds()
                )
        except Exception as e:
            return ExecutionResult(
                success=False,
                errors=str(e),
                execution_time=(datetime.now() - start_time).total_seconds()
            )
        finally:
            # Clean up temp directory
            try:
                if 'temp_dir' in locals():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass

    def analyze_types(self, code: str) -> Dict[str, Any]:
        """Analyze TypeScript code for type safety"""
        analysis = {
            'any_types': 0,
            'null_checks': 0,
            'generic_usage': 0,
            'interface_compliance': 0
        }

        # Count 'any' types
        analysis['any_types'] = len(re.findall(r'\bany\b', code))

        # Count null checks
        null_patterns = [r'!== null', r'!= null', r'!== undefined', r'!= undefined', r'&&']
        analysis['null_checks'] = sum(len(re.findall(pattern, code)) for pattern in null_patterns)

        # Count generics
        analysis['generic_usage'] = len(re.findall(r'<\w+>', code))

        # Check interface implementation
        interface_matches = re.findall(r'implements\s+(\w+)', code)
        analysis['interface_compliance'] = len(interface_matches)

        return analysis


class TestValidator:
    """Validator for Jest test execution and coverage analysis"""

    def __init__(self):
        self.monitor = structured_log(__name__)
        self.test_directory = tempfile.mkdtemp(prefix='ts_validation_')
        self.jest_config = {
            'collectCoverage': True,
            'coverageReporters': ['json', 'text'],
            'testTimeout': int(os.getenv('JEST_TIMEOUT', '10000')),
            'setupFilesAfterEnv': [],
            'testEnvironment': 'node'
        }

    def run_tests(self, test_code: str, source_code: str) -> TestResult:
        """Run Jest tests and collect results"""
        start_time = datetime.now()

        try:
            temp_dir = tempfile.mkdtemp(dir='/tmp')
            # Create test files
            source_file = os.path.join(temp_dir, 'source.ts')
            test_file = os.path.join(temp_dir, 'source.test.ts')
            jest_config_file = os.path.join(temp_dir, 'jest.config.js')

            # Write source code
            with open(source_file, 'w') as f:
                f.write(source_code)

            # Write test code
            with open(test_file, 'w') as f:
                f.write(test_code)

            # Write Jest config
            with open(jest_config_file, 'w') as f:
                f.write(f'module.exports = {json.dumps(self.jest_config, indent=2)};')

            # Create package.json for Jest
            package_json = {
                "name": "test-validation",
                "version": "1.0.0",
                "scripts": {
                    "test": "jest"
                },
                "devDependencies": {
                    "@types/jest": "^29.0.0",
                    "jest": "^29.0.0",
                    "ts-jest": "^29.0.0",
                    "@types/node": "^20.0.0",
                    "typescript": "^5.0.0"
                }
            }

            with open(os.path.join(temp_dir, 'package.json'), 'w') as f:
                json.dump(package_json, f, indent=2)

            # Install dependencies (if npm available)
            try:
                subprocess.run(['npm', 'install'], cwd=temp_dir,
                              capture_output=True, timeout=60)
            except:
                self.monitor.warning("npm install failed, proceeding without dependencies")

            # Run Jest
            result = subprocess.run(
                ['npx', 'jest', '--config', jest_config_file, '--coverage', '--verbose'],
                cwd=temp_dir,
                capture_output=True,
                text=True,
                timeout=60
            )

            execution_time = (datetime.now() - start_time).total_seconds()

            # Check for mock timeout in tests
            if hasattr(result, 'returncode') and "TimeoutExpired" in str(result.returncode):
                return TestResult(
                    success=False,
                    errors="Test execution timeout",
                    execution_time=execution_time
                )

            # Parse results
            test_result = self._parse_jest_output(result.stdout, result.stderr)
            coverage = self._parse_coverage_report(temp_dir)

            return TestResult(
                success=result.returncode == 0,
                output=result.stdout,
                errors=result.stderr,
                coverage=coverage,
                execution_time=execution_time,
                test_count=test_result['total'],
                passed_count=test_result['passed'],
                failed_count=test_result['failed']
            )

        except Exception as e:
            if isinstance(e, subprocess.TimeoutExpired) or "timeout" in str(e).lower() or "expired" in str(e).lower():
                return TestResult(
                    success=False,
                    errors="Test execution timeout",
                    execution_time=(datetime.now() - start_time).total_seconds()
                )
            else:
                return TestResult(
                    success=False,
                    errors=str(e),
                    execution_time=(datetime.now() - start_time).total_seconds()
                )
        except Exception as e:
            return TestResult(
                success=False,
                errors=str(e),
                execution_time=(datetime.now() - start_time).total_seconds()
            )

    def _parse_jest_output(self, stdout: str, stderr: str) -> Dict[str, int]:
        """Parse Jest test output"""
        result = {'total': 0, 'passed': 0, 'failed': 0}

        # Try JSON parsing first
        if stdout.strip().startswith('{'):
            try:
                data = json.loads(stdout)
                for test_result in data.get('testResults', []):
                    for assertion in test_result.get('assertionResults', []):
                        result['total'] += 1
                        if assertion.get('status') == 'passed':
                            result['passed'] += 1
                        elif assertion.get('status') == 'failed':
                            result['failed'] += 1
                return result
            except json.JSONDecodeError:
                pass

        # Fallback to text parsing
        lines = stdout.split('\n')
        for line in lines:
            if 'Tests:' in line and 'passed' in line:
                # Extract numbers from "Tests: 3 passed, 1 failed"
                match = re.search(r'Tests:\s*(\d+)\s*passed,\s*(\d+)\s*failed', line)
                if match:
                    result['passed'] = int(match.group(1))
                    result['failed'] = int(match.group(2))
                    result['total'] = result['passed'] + result['failed']
                break

        return result

    def _parse_coverage_report(self, temp_dir) -> Dict[str, Any]:
        """Parse Jest coverage report"""
        coverage_file = os.path.join(temp_dir, 'coverage', 'coverage-summary.json')

        if os.path.exists(coverage_file):
            try:
                with open(coverage_file, 'r') as f:
                    return json.load(f)
            except:
                pass

        return {}

    def analyze_coverage(self, coverage_data: Dict[str, Any]) -> CoverageReport:
        """Analyze test coverage metrics"""
        if not coverage_data or 'total' not in coverage_data:
            return CoverageReport()

        total = coverage_data['total']

        line_coverage = total.get('lines', {}).get('pct', 0.0)
        branch_coverage = total.get('branches', {}).get('pct', 0.0)
        function_coverage = total.get('functions', {}).get('pct', 0.0)

        # Calculate quality score (weighted average)
        quality_score = (
            line_coverage * 0.5 +
            branch_coverage * 0.3 +
            function_coverage * 0.2
        ) / 100.0

        # Find uncovered lines (simplified)
        uncovered_lines = []
        if 'source.ts' in coverage_data:
            source_coverage = coverage_data['source.ts']
            for line_num, coverage in source_coverage.get('lines', {}).items():
                if coverage == 0:
                    uncovered_lines.append(int(line_num))

        return CoverageReport(
            line_coverage=line_coverage,
            branch_coverage=branch_coverage,
            function_coverage=function_coverage,
            uncovered_lines=uncovered_lines,
            quality_score=quality_score
        )

    def _write_test_files(self, test_code: str, source_code: str) -> Tuple[str, str]:
        """Write test and source files to temporary directory"""
        source_file = os.path.join(self.test_directory, 'source.ts')
        test_file = os.path.join(self.test_directory, 'source.test.ts')

        # Write source code
        with open(source_file, 'w') as f:
            f.write(source_code)

        # Write test code
        with open(test_file, 'w') as f:
            f.write(test_code)

        return source_file, test_file
class LangChainValidator:
    """Validator for LangChain best practices and patterns"""

    def __init__(self):
        self.monitor = structured_log(__name__)

    def validate_patterns(self, code: str) -> ChainValidation:
        """Validate LangChain chain composition and patterns"""
        # Use string/regex patterns for TypeScript code instead of AST
        return ChainValidation(
            uses_lcel=self._check_lcel_usage(code),
            proper_error_handling=self._validate_error_handling(code),
            state_management=self._analyze_state_flow(code),
            tool_integration=self._check_tool_usage(code),
            composability_score=self._calculate_composability_score(code)
        )

    def _check_lcel_usage(self, code: str) -> bool:
        """Check for LangChain Expression Language (LCEL) usage"""
        lcel_patterns = ['pipe', r'\.pipe\(', 'RunnableSequence', 'RunnableParallel']

        for pattern in lcel_patterns:
            if re.search(pattern, code):
                return True

        return False

    def _validate_error_handling(self, code: str) -> bool:
        """Validate error handling patterns"""
        # Check for try-except blocks in Python or try-catch in TypeScript/JavaScript
        try_except_pattern = r'try\s*[:\{][\s\S]*?(except|catch)'
        return bool(re.search(try_except_pattern, code, re.DOTALL | re.IGNORECASE))

    def _analyze_state_flow(self, code: str) -> bool:
        """Analyze state management patterns"""
        # Check for state-related classes and immutability patterns
        state_class_pattern = r'class.*State'
        immutable_patterns = [r'readonly', r'const', r'interface.*State', r'@dataclass', r'frozen\s*=\s*True']

        has_state_class = bool(re.search(state_class_pattern, code))
        has_immutable = any(re.search(pattern, code) for pattern in immutable_patterns)

        return has_state_class and has_immutable

    def _check_tool_usage(self, code: str) -> bool:
        """Check for proper tool integration patterns"""
        # Check for tool patterns in TypeScript
        tool_patterns = [r'@tool', r'StructuredTool', r'bind_tools', r'with_types']

        return any(re.search(pattern, code) for pattern in tool_patterns)

    def _calculate_composability_score(self, code: str) -> float:
        """Calculate composability score (0-10)"""
        score = 0.0

        # LCEL usage (+3)
        if self._check_lcel_usage(code):
            score += 3.0

        # Error handling (+2)
        if self._validate_error_handling(code):
            score += 2.0

        # State management (+2)
        if self._analyze_state_flow(code):
            score += 2.0

        # Tool integration (+2)
        if self._check_tool_usage(code):
            score += 2.0

        # Circuit breaker usage (+1)
        if 'circuit_breaker' in code.lower() or 'CircuitBreaker' in code:
            score += 1.0

        return min(10.0, score)

    def validate_error_patterns(self, code: str) -> ErrorValidation:
        """Validate error handling patterns in detail"""
        try:
            ast_tree = ast.parse(code)
        except SyntaxError:
            return ErrorValidation()

        try_catch_blocks = self._find_try_catch_blocks(ast_tree)
        circuit_breakers = self._find_circuit_breakers(code)
        fallback_strategies = self._find_fallback_patterns(code)

        return ErrorValidation(
            try_catch_coverage=self._calculate_coverage(try_catch_blocks),
            circuit_breaker_usage=len(circuit_breakers) > 0,
            fallback_strategies=len(fallback_strategies),
            error_propagation=self._analyze_error_flow(code)
        )

    def _find_try_catch_blocks(self, ast_tree: ast.AST) -> List[ast.Try]:
        """Find all try-catch blocks"""
        return [node for node in ast.walk(ast_tree) if isinstance(node, ast.Try)]

    def _find_circuit_breakers(self, code: str) -> List[str]:
        """Find circuit breaker usage"""
        patterns = [r'circuit.?breaker', r'CircuitBreaker']
        matches = []
        for pattern in patterns:
            matches.extend(re.findall(pattern, code, re.IGNORECASE))
        return matches

    def _find_fallback_patterns(self, code: str) -> List[str]:
        """Find fallback strategy patterns"""
        patterns = [r'fallback', r'alternative', r'retry', r'exponential.?backoff']
        matches = []
        for pattern in patterns:
            matches.extend(re.findall(pattern, code, re.IGNORECASE))
        return matches

    def _calculate_coverage(self, try_blocks: List[ast.Try]) -> float:
        """Calculate error handling coverage"""
        if not try_blocks:
            return 0.0
        # More accurate: check for comprehensive error handling
        coverage = 0.0
        for try_block in try_blocks:
            if try_block.handlers:  # Has except blocks
                coverage += 0.3
            if try_block.orelse:  # Has else block
                coverage += 0.2
            if try_block.finalbody:  # Has finally block
                coverage += 0.2
            coverage += 0.3  # Base for having try
        return min(1.0, coverage)

    def _analyze_error_flow(self, code: str) -> bool:
        """Analyze error propagation patterns"""
        # Check for proper error raising/re-raising
        error_patterns = [r'raise', r'except.*:', r'finally']
        return any(re.search(pattern, code) for pattern in error_patterns)

    def validate_state_handling(self, code: str) -> StateValidation:
        """Validate state management implementation"""
        try:
            ast_tree = ast.parse(code)
        except SyntaxError:
            return StateValidation()

        state_classes = self._find_state_classes(ast_tree)
        immutability_patterns = self._check_immutability(code)
        transformation_methods = self._find_state_transformers(ast_tree)

        return StateValidation(
            immutable_state=len(state_classes) > 0,
            proper_transformations=len(state_classes) > 0 and len(transformation_methods) > 0,
            state_flow=self._analyze_state_flow(code),
            dataclasses_usage=self._check_dataclass_usage(ast_tree)
        )

    def _find_state_classes(self, ast_tree: ast.AST) -> List[ast.ClassDef]:
        """Find state-related classes"""
        return [node for node in ast.walk(ast_tree)
                if isinstance(node, ast.ClassDef) and 'state' in node.name.lower()]

    def _check_immutability(self, code: str) -> bool:
        """Check for immutability patterns"""
        immutable_patterns = [r'frozen\s*=\s*True', r'@dataclass\(frozen', r'NamedTuple']
        return any(re.search(pattern, code) for pattern in immutable_patterns)

    def _find_state_transformers(self, ast_tree: ast.AST) -> List[ast.FunctionDef]:
        """Find state transformation methods"""
        transformers = []
        for node in ast.walk(ast_tree):
            if isinstance(node, ast.FunctionDef):
                # Check method name patterns
                if any(keyword in node.name.lower() for keyword in ['transform', 'update', 'with_', 'add_', 'advance', 'increment', 'decrement', 'reset', 'set_']):
                    transformers.append(node)
                # Check if method has return statement (indicating transformation)
                elif any(isinstance(stmt, ast.Return) for stmt in node.body if node.body):
                    transformers.append(node)
        return transformers

    def _check_dataclass_usage(self, ast_tree: ast.AST) -> bool:
        """Check for dataclass usage"""
        for node in ast.walk(ast_tree):
            if isinstance(node, ast.ClassDef):
                for decorator in node.decorator_list:
                    if isinstance(decorator, ast.Name) and decorator.id == 'dataclass':
                        return True
                    elif isinstance(decorator, ast.Call) and isinstance(decorator.func, ast.Name) and decorator.func.id == 'dataclass':
                        return True
        return False


class QualityScorer:
    """Calculates overall quality scores for validation results"""

    WEIGHTS = {
        'safety': 0.4,
        'test_coverage': 0.3,
        'pattern_compliance': 0.3
    }

    def calculate_score(self, report: ValidationReport) -> float:
        """Calculate weighted quality score"""
        score = 0.0

        if report.safety_check:
            safety_score = 1.0 if report.safety_check.success else 0.0
            score += safety_score * self.WEIGHTS['safety']

        if report.test_results:
            # Use coverage percentage as test score
            coverage_pct = report.test_results.coverage.get('total', {}).get('lines', {}).get('pct', 0)
            test_score = coverage_pct / 100.0
            score += test_score * self.WEIGHTS['test_coverage']

        if report.pattern_validation:
            pattern_score = report.pattern_validation.composability_score / 10.0
            score += pattern_score * self.WEIGHTS['pattern_compliance']

        return score * 100.0

    def assess_risk(self, score: float) -> RiskLevel:
        """Determine risk level based on score"""
        if score >= 80:
            return RiskLevel.LOW
        elif score >= 60:
            return RiskLevel.MEDIUM
        elif score >= 40:
            return RiskLevel.HIGH
        else:
            return RiskLevel.CRITICAL


class LLMCodeValidationPipeline:
    """Complete validation pipeline for LLM-generated code"""

    def __init__(self):
        self.monitor = structured_log(__name__)
        self.ts_validator = TypeScriptValidator()
        self.test_validator = TestValidator()
        self.pattern_validator = LangChainValidator()
        self.quality_scorer = QualityScorer()

    def validate_typescript_code(
        self,
        code: str,
        tests: str,
        context: Dict[str, Any] = None
    ) -> ValidationReport:
        """Validate TypeScript code with tests"""
        context = context or {}
        code_hash = hashlib.sha256(code.encode()).hexdigest()[:8]

        report = ValidationReport(code_hash=code_hash)

        try:
            # Phase 1: Safety validation (compilation + runtime)
            self.monitor.info("Starting safety validation")
            compile_result = self.ts_validator.validate_compilation(code)
            if compile_result.success:
                safety_result = self.ts_validator.validate_runtime_safety(code)
                report.safety_check = safety_result
            else:
                # If compilation fails, create failed safety result
                report.safety_check = ExecutionResult(
                    success=False,
                    errors='\n'.join(compile_result.errors)
                )

            # Phase 2: Test execution
            self.monitor.info("Starting test validation")
            test_result = self.test_validator.run_tests(tests, code)
            report.test_results = test_result

            # Phase 3: Pattern validation (if Python agentics code)
            if context.get('is_agentics_code'):
                self.monitor.info("Starting pattern validation")
                pattern_result = self.pattern_validator.validate_patterns(code)
                report.pattern_validation = pattern_result

            # Phase 4: Calculate overall score and risk
            report.overall_score = self.quality_scorer.calculate_score(report)
            report.risk_level = self.quality_scorer.assess_risk(report.overall_score)

            # Phase 5: Generate recommendations
            self._generate_recommendations(report)

        except Exception as e:
            self.monitor.error(f"Validation pipeline failed: {str(e)}")
            report.critical_issues.append(f"Validation pipeline error: {str(e)}")
            report.overall_score = 0.0
            report.risk_level = RiskLevel.CRITICAL

        return report

    def _generate_recommendations(self, report: ValidationReport):
        """Generate recommendations based on validation results"""
        # Safety issues
        if report.safety_check and not report.safety_check.success:
            report.critical_issues.append("Code execution failed - review runtime errors")
            if report.safety_check.timeout:
                report.critical_issues.append("Code execution timed out - possible infinite loop")

        # Test issues
        if report.test_results:
            if not report.test_results.success:
                report.critical_issues.append(f"Tests failed: {report.test_results.failed_count} failing tests")
            if report.test_results.coverage.get('total', {}).get('lines', {}).get('pct', 100) < 80:
                report.warnings.append("Test coverage below 80%")

        # Pattern issues
        if report.pattern_validation:
            if not report.pattern_validation.uses_lcel:
                report.suggestions.append("Consider using LangChain Expression Language (LCEL) for better composability")
            if not report.pattern_validation.proper_error_handling:
                report.warnings.append("Improve error handling patterns")
            if not report.pattern_validation.state_management:
                report.suggestions.append("Implement proper state management patterns")


# Global instances
code_validator = LLMCodeValidationPipeline()

def validate_generated_code(code: str, tests: str, context: Dict[str, Any] = None) -> ValidationReport:
    """Global function for code validation"""
    return code_validator.validate_typescript_code(code, tests, context)

def get_code_validator() -> LLMCodeValidationPipeline:
    """Get the global code validator instance"""
    return code_validator