"""
Examples and Usage Guide for the LLM Test Suite Validator

This module provides comprehensive examples showing how to use the test suite validator
to validate LLM-generated TypeScript code and tests.
"""

from .test_suite import (
    validate_llm_test_suite,
    generate_test_suite_report,
    LLMTestSuiteValidator,
    TestSuiteValidationResult
)


def example_basic_validation():
    """Basic example of validating LLM-generated code and tests"""

    # Sample TypeScript code generated by an LLM
    generated_code = """
export interface User {
    id: string;
    name: string;
    email: string;
}

export class UserService {
    private users: Map<string, User> = new Map();

    async createUser(userData: { id: string; name: string; email: string }): Promise<void> {
        if (!userData.id || !userData.name || !userData.email) {
            throw new Error('Invalid user data: missing required fields');
        }

        const user: User = {
            id: userData.id,
            name: userData.name.trim(),
            email: userData.email.toLowerCase()
        };

        this.users.set(user.id, user);
    }

    async getUser(id: string): Promise<User | null> {
        if (!id) {
            throw new Error('User ID is required');
        }
        return this.users.get(id) || null;
    }

    async getAllUsers(): Promise<User[]> {
        return Array.from(this.users.values());
    }

    async deleteUser(id: string): Promise<boolean> {
        if (!id) {
            throw new Error('User ID is required');
        }
        return this.users.delete(id);
    }
}
"""

    # Sample tests generated by an LLM
    generated_tests = """
import { UserService } from './source';

describe('UserService', () => {
    let userService: UserService;

    beforeEach(() => {
        userService = new UserService();
    });

    describe('createUser', () => {
        it('should create a user successfully with valid data', async () => {
            const userData = {
                id: 'user-123',
                name: 'John Doe',
                email: 'john.doe@example.com'
            };

            await expect(userService.createUser(userData)).resolves.toBeUndefined();

            const createdUser = await userService.getUser('user-123');
            expect(createdUser).toEqual({
                id: 'user-123',
                name: 'John Doe',
                email: 'john.doe@example.com'
            });
        });

        it('should throw error when id is missing', async () => {
            const invalidData = {
                id: '',
                name: 'John Doe',
                email: 'john@example.com'
            };

            await expect(userService.createUser(invalidData)).rejects.toThrow('Invalid user data');
        });

        it('should throw error when name is missing', async () => {
            const invalidData = {
                id: 'user-123',
                name: '',
                email: 'john@example.com'
            };

            await expect(userService.createUser(invalidData)).rejects.toThrow('Invalid user data');
        });

        it('should throw error when email is missing', async () => {
            const invalidData = {
                id: 'user-123',
                name: 'John Doe',
                email: ''
            };

            await expect(userService.createUser(invalidData)).rejects.toThrow('Invalid user data');
        });

        it('should trim whitespace from name', async () => {
            const userData = {
                id: 'user-123',
                name: '  John Doe  ',
                email: 'john@example.com'
            };

            await userService.createUser(userData);
            const user = await userService.getUser('user-123');

            expect(user?.name).toBe('John Doe');
        });

        it('should convert email to lowercase', async () => {
            const userData = {
                id: 'user-123',
                name: 'John Doe',
                email: 'JOHN.DOE@EXAMPLE.COM'
            };

            await userService.createUser(userData);
            const user = await userService.getUser('user-123');

            expect(user?.email).toBe('john.doe@example.com');
        });
    });

    describe('getUser', () => {
        it('should return user when user exists', async () => {
            const userData = {
                id: 'user-123',
                name: 'John Doe',
                email: 'john@example.com'
            };

            await userService.createUser(userData);
            const user = await userService.getUser('user-123');

            expect(user).toEqual(userData);
        });

        it('should return null when user does not exist', async () => {
            const user = await userService.getUser('nonexistent');
            expect(user).toBeNull();
        });

        it('should throw error when id is empty', async () => {
            await expect(userService.getUser('')).rejects.toThrow('User ID is required');
        });
    });

    describe('getAllUsers', () => {
        it('should return all users', async () => {
            const users = [
                { id: '1', name: 'User 1', email: 'user1@test.com' },
                { id: '2', name: 'User 2', email: 'user2@test.com' },
                { id: '3', name: 'User 3', email: 'user3@test.com' }
            ];

            for (const user of users) {
                await userService.createUser(user);
            }

            const allUsers = await userService.getAllUsers();
            expect(allUsers).toHaveLength(3);
            expect(allUsers).toEqual(expect.arrayContaining(users));
        });

        it('should return empty array when no users exist', async () => {
            const allUsers = await userService.getAllUsers();
            expect(allUsers).toEqual([]);
        });
    });

    describe('deleteUser', () => {
        it('should delete existing user and return true', async () => {
            const userData = { id: '1', name: 'User 1', email: 'user1@test.com' };
            await userService.createUser(userData);

            const deleted = await userService.deleteUser('1');
            expect(deleted).toBe(true);

            const user = await userService.getUser('1');
            expect(user).toBeNull();
        });

        it('should return false when user does not exist', async () => {
            const deleted = await userService.deleteUser('nonexistent');
            expect(deleted).toBe(false);
        });

        it('should throw error when id is empty', async () => {
            await expect(userService.deleteUser('')).rejects.toThrow('User ID is required');
        });
    });
});
"""

    print("=== Basic Test Suite Validation Example ===\n")

    # Validate the generated code and tests
    result = validate_llm_test_suite(
        generated_code,
        generated_tests,
        context={
            "language": "typescript",
            "framework": "node",
            "test_framework": "jest",
            "domain": "user_management"
        }
    )

    # Generate and print the report
    report = generate_test_suite_report(generated_code, generated_tests, result)
    print(report)

    return result


def example_langchain_agent_validation():
    """Example of validating LangChain agentics code"""

    # Sample LangChain agent code
    agent_code = """
from langchain_core.runnables import RunnableLambda, RunnableSequence
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from typing import Dict, Any, List
from dataclasses import dataclass
import asyncio

@dataclass(frozen=True)
class AgentState:
    messages: List[Dict[str, Any]]
    context: Dict[str, Any] = None
    tools_used: List[str] = None

    def with_message(self, message: Dict[str, Any]) -> 'AgentState':
        return AgentState(
            messages=self.messages + [message],
            context=self.context,
            tools_used=self.tools_used
        )

    def with_context(self, context: Dict[str, Any]) -> 'AgentState':
        return AgentState(
            messages=self.messages,
            context=context,
            tools_used=self.tools_used
        )

    def with_tool_used(self, tool_name: str) -> 'AgentState':
        current_tools = self.tools_used or []
        return AgentState(
            messages=self.messages,
            context=self.context,
            tools_used=current_tools + [tool_name]
        )

@tool
def search_documents(query: str) -> str:
    \"\"\"Search through documents for relevant information.\"\"\"
    try:
        # Simulate document search
        results = f"Found documents related to: {query}"
        return results
    except Exception as e:
        raise ValueError(f"Document search failed: {str(e)}")

@tool
def calculate_metrics(data: Dict[str, Any]) -> Dict[str, Any]:
    \"\"\"Calculate various metrics from input data.\"\"\"
    try:
        if not data or not isinstance(data, dict):
            raise ValueError("Invalid data provided")

        metrics = {
            "total_items": len(data),
            "processed_at": "2024-01-01T00:00:00Z",
            "status": "completed"
        }
        return metrics
    except Exception as e:
        raise ValueError(f"Metrics calculation failed: {str(e)}")

class DocumentAgent:
    \"\"\"Agent for processing documents and answering questions\"\"\"

    def __init__(self, llm):
        self.llm = llm
        self.tools = [search_documents, calculate_metrics]
        self.processing_chain = self._build_processing_chain()

    def _build_processing_chain(self) -> RunnableSequence:
        \"\"\"Build the main processing chain using LCEL\"\"\"

        # Step 1: Analyze the query
        analyze_query = RunnableLambda(self._analyze_query)

        # Step 2: Determine which tools to use
        select_tools = RunnableLambda(self._select_tools)

        # Step 3: Execute tools in parallel if needed
        execute_tools = RunnableLambda(self._execute_tools)

        # Step 4: Generate final response
        generate_response = RunnableLambda(self._generate_response)

        return analyze_query | select_tools | execute_tools | generate_response

    def _analyze_query(self, state: AgentState) -> AgentState:
        \"\"\"Analyze the input query to understand requirements\"\"\"
        try:
            if not state.messages:
                raise ValueError("No messages provided")

            last_message = state.messages[-1]
            query = last_message.get('content', '')

            analysis = {
                "query_type": "search" if "search" in query.lower() else "analysis",
                "requires_tools": len(query.split()) > 3,
                "complexity": "high" if len(query) > 100 else "low"
            }

            return state.with_context({"query_analysis": analysis})
        except Exception as e:
            raise ValueError(f"Query analysis failed: {str(e)}")

    def _select_tools(self, state: AgentState) -> AgentState:
        \"\"\"Select appropriate tools based on query analysis\"\"\"
        try:
            analysis = state.context.get("query_analysis", {})
            selected_tools = []

            if analysis.get("query_type") == "search":
                selected_tools.append("search_documents")
            if analysis.get("requires_tools"):
                selected_tools.append("calculate_metrics")

            return state.with_context({
                **state.context,
                "selected_tools": selected_tools
            })
        except Exception as e:
            raise ValueError(f"Tool selection failed: {str(e)}")

    def _execute_tools(self, state: AgentState) -> AgentState:
        \"\"\"Execute selected tools and collect results\"\"\"
        try:
            selected_tools = state.context.get("selected_tools", [])
            tool_results = {}

            for tool_name in selected_tools:
                if tool_name == "search_documents":
                    query = state.messages[-1].get('content', '')
                    result = search_documents.invoke({"query": query})
                    tool_results[tool_name] = result
                    state = state.with_tool_used(tool_name)

                elif tool_name == "calculate_metrics":
                    data = state.context.get("query_analysis", {})
                    result = calculate_metrics.invoke({"data": data})
                    tool_results[tool_name] = result
                    state = state.with_tool_used(tool_name)

            return state.with_context({
                **state.context,
                "tool_results": tool_results
            })
        except Exception as e:
            raise ValueError(f"Tool execution failed: {str(e)}")

    def _generate_response(self, state: AgentState) -> Dict[str, Any]:
        \"\"\"Generate final response based on tool results\"\"\"
        try:
            tool_results = state.context.get("tool_results", {})
            tools_used = state.tools_used or []

            response = {
                "answer": f"Processed query using {len(tools_used)} tools",
                "tool_results": tool_results,
                "tools_used": tools_used,
                "confidence": 0.85,
                "processing_time": "1.2s"
            }

            return response
        except Exception as e:
            raise ValueError(f"Response generation failed: {str(e)}")

    async def process_query(self, query: str) -> Dict[str, Any]:
        \"\"\"Main entry point for processing user queries\"\"\"
        try:
            initial_message = {"role": "user", "content": query}
            initial_state = AgentState(messages=[initial_message])

            result = await self.processing_chain.ainvoke(initial_state)
            return result
        except Exception as e:
            return {
                "error": str(e),
                "status": "failed",
                "tools_used": []
            }
"""

    # Sample tests for the agent
    agent_tests = """
import { DocumentAgent } from './agent';
import { search_documents, calculate_metrics } from './tools';

describe('DocumentAgent', () => {
    let agent: DocumentAgent;
    let mockLLM: any;

    beforeEach(() => {
        mockLLM = {
            invoke: jest.fn().mockResolvedValue({ content: 'Mock response' })
        };
        agent = new DocumentAgent(mockLLM);
    });

    describe('process_query', () => {
        it('should process search queries successfully', async () => {
            const query = 'Search for documents about TypeScript';
            const result = await agent.process_query(query);

            expect(result).toHaveProperty('answer');
            expect(result).toHaveProperty('tool_results');
            expect(result.tools_used).toContain('search_documents');
            expect(result.confidence).toBeGreaterThan(0);
        });

        it('should handle complex queries with multiple tools', async () => {
            const query = 'Search for documents and calculate metrics for the results';
            const result = await agent.process_query(query);

            expect(result.tools_used).toContain('search_documents');
            expect(result.tools_used).toContain('calculate_metrics');
            expect(result.tool_results).toHaveProperty('search_documents');
            expect(result.tool_results).toHaveProperty('calculate_metrics');
        });

        it('should handle errors gracefully', async () => {
            // Test with invalid input
            const result = await agent.process_query('');

            expect(result).toHaveProperty('error');
            expect(result.status).toBe('failed');
        });

        it('should maintain conversation context', async () => {
            const query1 = 'Search for TypeScript docs';
            const result1 = await agent.process_query(query1);

            expect(result1.tools_used.length).toBeGreaterThan(0);

            // In a real scenario, context would be maintained
            // This is a simplified test
        });
    });

    describe('tool integration', () => {
        it('should use search_documents tool correctly', async () => {
            const query = 'find information about testing';
            const result = await agent.process_query(query);

            expect(result.tool_results).toHaveProperty('search_documents');
            expect(typeof result.tool_results.search_documents).toBe('string');
        });

        it('should use calculate_metrics tool for data analysis', async () => {
            const query = 'analyze this data and provide metrics';
            const result = await agent.process_query(query);

            expect(result.tool_results).toHaveProperty('calculate_metrics');
            expect(result.tool_results.calculate_metrics).toHaveProperty('total_items');
        });
    });

    describe('error handling', () => {
        it('should handle tool execution failures', async () => {
            // Test with a query that might cause tool failure
            const query = 'process invalid data';
            const result = await agent.process_query(query);

            // Should not throw but return error in result
            expect(result).toBeDefined();
            if (result.error) {
                expect(result.status).toBe('failed');
            }
        });

        it('should validate input parameters', async () => {
            const result = await agent.process_query(null as any);

            expect(result).toHaveProperty('error');
        });
    });
});
"""

    print("\n=== LangChain Agent Validation Example ===\n")

    # Validate the LangChain agent code
    result = validate_llm_test_suite(
        agent_code,
        agent_tests,
        context={
            "language": "python",
            "framework": "langchain",
            "type": "agentics_code",
            "domain": "document_processing"
        }
    )

    # Generate and print the report
    report = generate_test_suite_report(agent_code, agent_tests, result)
    print(report)

    return result


def example_advanced_validation_with_custom_context():
    """Example showing advanced validation with custom context and detailed analysis"""

    # Create validator instance for more control
    validator = LLMTestSuiteValidator()

    # Sample React component code
    react_code = """
import React, { useState, useEffect, useCallback } from 'react';
import { User } from './types';

interface UserListProps {
    users: User[];
    onUserSelect: (user: User) => void;
    loading?: boolean;
}

export const UserList: React.FC<UserListProps> = ({
    users,
    onUserSelect,
    loading = false
}) => {
    const [selectedUserId, setSelectedUserId] = useState<string | null>(null);
    const [filter, setFilter] = useState('');

    const filteredUsers = users.filter(user =>
        user.name.toLowerCase().includes(filter.toLowerCase()) ||
        user.email.toLowerCase().includes(filter.toLowerCase())
    );

    const handleUserClick = useCallback((user: User) => {
        setSelectedUserId(user.id);
        onUserSelect(user);
    }, [onUserSelect]);

    const clearSelection = useCallback(() => {
        setSelectedUserId(null);
    }, []);

    if (loading) {
        return <div className="loading">Loading users...</div>;
    }

    return (
        <div className="user-list">
            <div className="filter-container">
                <input
                    type="text"
                    placeholder="Filter users..."
                    value={filter}
                    onChange={(e) => setFilter(e.target.value)}
                    className="filter-input"
                />
                {selectedUserId && (
                    <button onClick={clearSelection} className="clear-button">
                        Clear Selection
                    </button>
                )}
            </div>

            <div className="users-container">
                {filteredUsers.length === 0 ? (
                    <div className="no-users">No users found</div>
                ) : (
                    filteredUsers.map(user => (
                        <div
                            key={user.id}
                            className={`user-item ${selectedUserId === user.id ? 'selected' : ''}`}
                            onClick={() => handleUserClick(user)}
                        >
                            <div className="user-name">{user.name}</div>
                            <div className="user-email">{user.email}</div>
                        </div>
                    ))
                )}
            </div>
        </div>
    );
};
"""

    # Sample React component tests
    react_tests = """
import React from 'react';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { UserList } from './UserList';
import { User } from './types';

const mockUsers: User[] = [
    { id: '1', name: 'John Doe', email: 'john@example.com' },
    { id: '2', name: 'Jane Smith', email: 'jane@example.com' },
    { id: '3', name: 'Bob Johnson', email: 'bob@example.com' }
];

describe('UserList', () => {
    const mockOnUserSelect = jest.fn();

    beforeEach(() => {
        mockOnUserSelect.mockClear();
    });

    it('renders all users when no filter is applied', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        expect(screen.getByText('John Doe')).toBeInTheDocument();
        expect(screen.getByText('Jane Smith')).toBeInTheDocument();
        expect(screen.getByText('Bob Johnson')).toBeInTheDocument();
    });

    it('filters users based on name', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        const filterInput = screen.getByPlaceholderText('Filter users...');
        fireEvent.change(filterInput, { target: { value: 'John' } });

        expect(screen.getByText('John Doe')).toBeInTheDocument();
        expect(screen.queryByText('Jane Smith')).not.toBeInTheDocument();
        expect(screen.queryByText('Bob Johnson')).not.toBeInTheDocument();
    });

    it('filters users based on email', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        const filterInput = screen.getByPlaceholderText('Filter users...');
        fireEvent.change(filterInput, { target: { value: 'example.com' } });

        expect(screen.getByText('John Doe')).toBeInTheDocument();
        expect(screen.getByText('Jane Smith')).toBeInTheDocument();
        expect(screen.getByText('Bob Johnson')).toBeInTheDocument();
    });

    it('calls onUserSelect when user is clicked', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        const userItem = screen.getByText('John Doe');
        fireEvent.click(userItem);

        expect(mockOnUserSelect).toHaveBeenCalledWith(mockUsers[0]);
        expect(mockOnUserSelect).toHaveBeenCalledTimes(1);
    });

    it('shows selected state when user is clicked', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        const userItem = screen.getByText('John Doe');
        fireEvent.click(userItem);

        // Check if the selected class is applied
        expect(userItem.closest('.user-item')).toHaveClass('selected');
    });

    it('clears selection when clear button is clicked', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        // Select a user first
        const userItem = screen.getByText('John Doe');
        fireEvent.click(userItem);

        // Click clear button
        const clearButton = screen.getByText('Clear Selection');
        fireEvent.click(clearButton);

        // Check that selected class is removed
        expect(userItem.closest('.user-item')).not.toHaveClass('selected');
    });

    it('shows loading state when loading prop is true', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
                loading={true}
            />
        );

        expect(screen.getByText('Loading users...')).toBeInTheDocument();
        expect(screen.queryByText('John Doe')).not.toBeInTheDocument();
    });

    it('shows no users message when filtered results are empty', () => {
        render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        const filterInput = screen.getByPlaceholderText('Filter users...');
        fireEvent.change(filterInput, { target: { value: 'nonexistent' } });

        expect(screen.getByText('No users found')).toBeInTheDocument();
    });

    it('shows clear selection button only when user is selected', () => {
        const { rerender } = render(
            <UserList
                users={mockUsers}
                onUserSelect={mockOnUserSelect}
            />
        );

        // Initially no clear button
        expect(screen.queryByText('Clear Selection')).not.toBeInTheDocument();

        // Select a user
        const userItem = screen.getByText('John Doe');
        fireEvent.click(userItem);

        // Now clear button should be visible
        expect(screen.getByText('Clear Selection')).toBeInTheDocument();
    });
});
"""

    print("\n=== Advanced React Component Validation Example ===\n")

    # Perform validation with custom context
    result = validator.validate_test_suite(
        react_code,
        react_tests,
        context={
            "language": "typescript",
            "framework": "react",
            "test_framework": "jest",
            "test_library": "testing-library",
            "component_type": "list_component",
            "hooks_used": ["useState", "useCallback"],
            "styling": "css_modules"
        },
        include_code_validator=True
    )

    # Generate detailed report
    report = validator.generate_detailed_report(react_code, react_tests, result)
    print(report)

    # Demonstrate programmatic access to results
    print("\n=== Programmatic Results Analysis ===\n")
    print(f"Overall Score: {result.overall_score:.1f}/100")
    print(f"Risk Level: {result.risk_level.value.upper()}")
    print(f"Code Execution Success: {result.code_execution.success}")
    print(f"Test Coverage: {result.test_execution.coverage_percentage:.1f}%")
    print(f"LangChain Compliance: {result.langchain_compliance.overall_compliance:.1f}/10")

    if result.critical_issues:
        print(f"\nCritical Issues ({len(result.critical_issues)}):")
        for issue in result.critical_issues:
            print(f"  - {issue}")

    if result.warnings:
        print(f"\nWarnings ({len(result.warnings)}):")
        for warning in result.warnings:
            print(f"  - {warning}")

    if result.suggestions:
        print(f"\nSuggestions ({len(result.suggestions)}):")
        for suggestion in result.suggestions:
            print(f"  - {suggestion}")

    return result


def example_error_handling_and_edge_cases():
    """Example demonstrating error handling and edge case validation"""

    print("\n=== Error Handling and Edge Cases Example ===\n")

    # Test with malformed code
    print("1. Testing with malformed code:")
    malformed_code = "export class { invalid syntax here }"
    malformed_tests = "describe('test', () => { it('fails', () => { expect().toBe(); }); });"

    result = validate_llm_test_suite(malformed_code, malformed_tests, include_code_validator=False)
    print(f"   Score: {result.overall_score:.1f}, Risk: {result.risk_level.value}")
    print(f"   Critical Issues: {len(result.critical_issues)}")

    # Test with empty inputs
    print("\n2. Testing with empty inputs:")
    result = validate_llm_test_suite("", "", include_code_validator=False)
    print(f"   Score: {result.overall_score:.1f}, Risk: {result.risk_level.value}")

    # Test with code that has no tests
    print("\n3. Testing with code but no meaningful tests:")
    code_with_no_tests = "export const add = (a, b) => a + b;"
    empty_tests = "describe('empty', () => {});"

    result = validate_llm_test_suite(code_with_no_tests, empty_tests, include_code_validator=False)
    print(f"   Score: {result.overall_score:.1f}, Risk: {result.risk_level.value}")
    print(f"   Test Count: {result.test_execution.total_tests}")

    print("\nError handling examples completed.")


def run_all_examples():
    """Run all examples to demonstrate the test suite capabilities"""

    print("LLM Test Suite Validator - Comprehensive Examples")
    print("=" * 60)

    try:
        # Run basic validation example
        basic_result = example_basic_validation()

        # Run LangChain agent validation
        agent_result = example_langchain_agent_validation()

        # Run advanced React validation
        react_result = example_advanced_validation_with_custom_context()

        # Run error handling examples
        example_error_handling_and_edge_cases()

        print("\n" + "=" * 60)
        print("SUMMARY OF ALL EXAMPLES")
        print("=" * 60)

        examples = [
            ("Basic TypeScript Service", basic_result),
            ("LangChain Agent", agent_result),
            ("React Component", react_result)
        ]

        for name, result in examples:
            print(f"\n{name}:")
            print(f"  Score: {result.overall_score:.1f}/100")
            print(f"  Risk: {result.risk_level.value.upper()}")
            print(f"  Code Success: {result.code_execution.success}")
            print(f"  Tests Run: {result.test_execution.total_tests}")
            print(f"  Coverage: {result.test_execution.coverage_percentage:.1f}%")

    except Exception as e:
        print(f"Error running examples: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_examples()